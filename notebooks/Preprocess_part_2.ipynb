{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:41:32.980134Z",
     "start_time": "2022-08-23T05:41:32.403384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import openpyxl\n",
    "import os\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:41:32.989143Z",
     "start_time": "2022-08-23T05:41:32.982793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in taxi_zones lookup table\n",
    "zones = pd.read_csv(\"../data/raw/external_data_and_taxi_zones/taxi_zone_lookup.csv\")\n",
    "zones = zones.drop([263, 264]) # drop the unknown zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess property sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:22.905587Z",
     "start_time": "2022-08-23T05:41:32.993309Z"
    }
   },
   "outputs": [],
   "source": [
    "relative_directory = '../data/raw/external_data_and_taxi_zones/'\n",
    "\n",
    "all_files = os.listdir(\"../data/raw/external_data_and_taxi_zones/\")    \n",
    "property_sale_files = list(filter(lambda x: x.endswith('.xlsx'), all_files))\n",
    "\n",
    "header = ['BOROUGH', 'NEIGHBORHOOD','BUILDING CLASS CATEGORY', \n",
    "          'TAX CLASS AT PRESENT', 'BLOCK', 'LOT', \n",
    "          'EASE-MENT', 'BUILDING CLASS AT PRESENT', 'ADDRESS',\n",
    "          'APARTMENT NUMBER', 'ZIP CODE', 'RESIDENTIAL UNITS',\n",
    "          'COMMERCIAL UNITS', 'TOTAL UNITS', 'LAND SQUARE FEET',\n",
    "          'GROSS SQUARE FEET', 'YEAR BUILT', 'TAX CLASS AT TIME OF SALE',\n",
    "          'BUILDING CLASS AT TIME OF SALE', 'SALE PRICE', 'SALE DATE']\n",
    "\n",
    "\n",
    "file_names = [relative_directory + file for file in property_sale_files]\n",
    "\n",
    "\n",
    "def read_xlxs_for_mapping(data):\n",
    "    return pd.read_excel(data, \n",
    "                        names = header, \n",
    "                        parse_dates = ['SALE DATE', ],\n",
    "                        engine = 'openpyxl')\n",
    "\n",
    "property_sales = pd.concat(map(read_xlxs_for_mapping, file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:22.913319Z",
     "start_time": "2022-08-23T05:42:22.908605Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data, columns):\n",
    "    '''\n",
    "    remove outliers from data that is 1.5 iqr away from q1 or q3\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    q1 = np.array([np.quantile(data[column], 0.25) for column in columns])\n",
    "    q3 = np.array([np.quantile(data[column], 0.75) for column in columns])\n",
    "    iqr = q3 - q1\n",
    "    for i in range(len(columns)):\n",
    "        column = columns[i]\n",
    "        new_data = new_data[(new_data[column] > q1[i] - 1.5 * iqr[i]) & (new_data[column] < q3[i] + 1.5 * iqr[i])]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:23.065788Z",
     "start_time": "2022-08-23T05:42:22.915101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 206017 entries, 0 to 21716\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count   Dtype \n",
      "---  ------                          --------------   ----- \n",
      " 0   BOROUGH                         152654 non-null  object\n",
      " 1   NEIGHBORHOOD                    152614 non-null  object\n",
      " 2   BUILDING CLASS CATEGORY         152614 non-null  object\n",
      " 3   TAX CLASS AT PRESENT            152383 non-null  object\n",
      " 4   BLOCK                           152614 non-null  object\n",
      " 5   LOT                             152614 non-null  object\n",
      " 6   EASE-MENT                       10 non-null      object\n",
      " 7   BUILDING CLASS AT PRESENT       152383 non-null  object\n",
      " 8   ADDRESS                         152614 non-null  object\n",
      " 9   APARTMENT NUMBER                33410 non-null   object\n",
      " 10  ZIP CODE                        152589 non-null  object\n",
      " 11  RESIDENTIAL UNITS               122566 non-null  object\n",
      " 12  COMMERCIAL UNITS                122566 non-null  object\n",
      " 13  TOTAL UNITS                     122566 non-null  object\n",
      " 14  LAND SQUARE FEET                122566 non-null  object\n",
      " 15  GROSS SQUARE FEET               122566 non-null  object\n",
      " 16  YEAR BUILT                      141743 non-null  object\n",
      " 17  TAX CLASS AT TIME OF SALE       152614 non-null  object\n",
      " 18  BUILDING CLASS AT TIME OF SALE  152614 non-null  object\n",
      " 19  SALE PRICE                      152614 non-null  object\n",
      " 20  SALE DATE                       206012 non-null  object\n",
      "dtypes: object(21)\n",
      "memory usage: 34.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "property_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:23.605910Z",
     "start_time": "2022-08-23T05:42:23.067415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove irrelevant rows\n",
    "property_sales = property_sales[property_sales['BOROUGH'].isin(['1', '2', '3', '4', '5'])]\n",
    "\n",
    "# Drop rows with NaN valus in required fields\n",
    "property_sales.dropna(how='any', subset=['BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS AT PRESENT',\n",
    "                                         'TOTAL UNITS', 'GROSS SQUARE FEET', 'BUILDING CLASS AT TIME OF SALE', \n",
    "                                         'SALE PRICE', 'SALE DATE'], inplace=True)\n",
    "\n",
    "# Change data type\n",
    "property_sales['BOROUGH'] = property_sales['BOROUGH'].astype(int)\n",
    "property_sales['SALE DATE'] = pd.to_datetime(property_sales['SALE DATE'])\n",
    "property_sales['SALE PRICE'] = pd.to_numeric(property_sales['SALE PRICE'])\n",
    "property_sales['GROSS SQUARE FEET'] = pd.to_numeric(property_sales['GROSS SQUARE FEET'])\n",
    "property_sales['TOTAL UNITS'] = pd.to_numeric(property_sales['TOTAL UNITS'])\n",
    "\n",
    "\n",
    "# According to property sales data, 1 for man, 2 for bronx, 3 for brooklyn, 4 for queens, 5 for staten island\n",
    "borough_dict = {4: 'Queens', 2: 'Bronx', 1: 'Manhattan', 5: 'Staten Island', 3: 'Brooklyn'}\n",
    "property_sales['BOROUGH'] = property_sales['BOROUGH'].apply(lambda x: borough_dict[x])\n",
    "\n",
    "# Assume the numeric data are normally distributed, remove the outliers\n",
    "property_sales = remove_outliers(property_sales, ['SALE PRICE', 'GROSS SQUARE FEET', 'TOTAL UNITS'])\n",
    "\n",
    "# Filter data that fits our analysis\n",
    "# Type ABCD are family dwellings and apartments, H is hotel\n",
    "condition = (property_sales['SALE DATE'] > '01-01-2019') & (property_sales['SALE DATE'] <= '29-02-2020') &\\\n",
    "            (property_sales['SALE PRICE'] > 9999) &\\\n",
    "            (property_sales['GROSS SQUARE FEET'] > 9) &\\\n",
    "            (property_sales['TOTAL UNITS'] > 0) &\\\n",
    "            (property_sales['BUILDING CLASS AT PRESENT'] == property_sales['BUILDING CLASS AT TIME OF SALE']) &\\\n",
    "            (property_sales['BUILDING CLASS AT TIME OF SALE'].str.contains('^[ABCDH]', regex=True))\n",
    "\n",
    "property_sales = property_sales.loc[condition]\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "property_sales.drop(columns = ['BUILDING CLASS CATEGORY', 'TAX CLASS AT PRESENT', 'BLOCK', \n",
    "                               'LOT', 'EASE-MENT', 'BUILDING CLASS AT PRESENT', \n",
    "                               'ADDRESS', 'APARTMENT NUMBER', 'ZIP CODE', \n",
    "                               'RESIDENTIAL UNITS', 'COMMERCIAL UNITS', \n",
    "                               'LAND SQUARE FEET', 'YEAR BUILT', 'TAX CLASS AT TIME OF SALE'], inplace=True)\n",
    "\n",
    "\n",
    "# Add 'PRICE PER UNIT' and 'PRICE PER SQUARE FEET' as features\n",
    "property_sales['PRICE PER UNIT'] = property_sales['SALE PRICE'] / property_sales['TOTAL UNITS']\n",
    "property_sales['PRICE PER SQUARE FEET'] = property_sales['SALE PRICE'] / property_sales['GROSS SQUARE FEET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:23.627910Z",
     "start_time": "2022-08-23T05:42:23.608131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "\n",
    "aggregated_sales_2019 = property_sales[(property_sales['SALE DATE'] > '01-01-2019') & \\\n",
    "                                       (property_sales['SALE DATE'] <= '31-12-2019')] \\\n",
    "                        .groupby(['NEIGHBORHOOD'], as_index = False).mean()\n",
    "\n",
    "aggregated_sales_2020 = property_sales[(property_sales['SALE DATE'] > '01-01-2020') & \\\n",
    "                                       (property_sales['SALE DATE'] <= '29-02-2020')] \\\n",
    "                        .groupby(['NEIGHBORHOOD'], as_index = False).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:24.263316Z",
     "start_time": "2022-08-23T05:42:23.629540Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data linkage has 234 matches'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link taxi zones data to property sales data based on their name of locations\n",
    "# for taxi zones data is the 'Zone', for property sales data is the 'Neighborhood'\n",
    "\n",
    "def t(name):\n",
    "    \"\"\"\n",
    "    return a transformed zone name that is splitted into several parts\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    name = re.sub('[\\/\\-0-9()]+', ' ', name)\n",
    "    names = name.split()\n",
    "    return names\n",
    "\n",
    "\n",
    "\n",
    "def link(name1, name_list_2, threshold_confidence):\n",
    "    \"\"\"\n",
    "    return a possible match of name1 from name_list_2\n",
    "    \"\"\"\n",
    "    confident_pairs = [[(name1, name2),\n",
    "                        len(set(t(name1)).intersection(t(name2))) \n",
    "                        / len(t(name1))\n",
    "                        ] \\\n",
    "                        for name2 in name_list_2]\n",
    "    max_confidence = max([confidence for pair, confidence in confident_pairs])\n",
    "    most_confident_pairs = [pair for pair, confidence in confident_pairs if confidence == max_confidence]\n",
    "    if max_confidence >= threshold_confidence:\n",
    "        # In the case where they are multiple matches, we choose the shortest match\n",
    "        # e.g., 'Astoria Park' will be matched to \"Astoria\" instead of \"Rego Park\" or \"Borogh Park\"\n",
    "        most_confident_pair = min(most_confident_pairs, key = lambda x: len(x[1]))\n",
    "        return most_confident_pair[1]\n",
    "\n",
    "\n",
    "# Find the corresponding neighborhood to each taxi zone\n",
    "zones_dict = {}\n",
    "for zone in zones['Zone'].unique():\n",
    "    zones_dict[zone] = link(zone, \n",
    "                            property_sales['NEIGHBORHOOD'].unique(), \n",
    "                            threshold_confidence = 0.25) # set a low threshold confidence to get high recall\n",
    "\n",
    "\n",
    "\n",
    "zones['Neighborhood'] = zones['Zone'].apply(lambda x: zones_dict[x])\n",
    "f'Data linkage has {len([i for i in zones_dict.values() if i != None])} matches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:24.273945Z",
     "start_time": "2022-08-23T05:42:24.265085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "      <td>AIRPORT LA GUARDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>JAMAICA BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>PELHAM GARDENS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>GRANT CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>ARDEN HEIGHTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone  \\\n",
       "0           1            EWR           Newark Airport          EWR   \n",
       "1           2         Queens              Jamaica Bay    Boro Zone   \n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone   \n",
       "3           4      Manhattan            Alphabet City  Yellow Zone   \n",
       "4           5  Staten Island            Arden Heights    Boro Zone   \n",
       "\n",
       "         Neighborhood  \n",
       "0  AIRPORT LA GUARDIA  \n",
       "1         JAMAICA BAY  \n",
       "2      PELHAM GARDENS  \n",
       "3          GRANT CITY  \n",
       "4       ARDEN HEIGHTS  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some errors still exist but not a big problem\n",
    "zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:24.296661Z",
     "start_time": "2022-08-23T05:42:24.275482Z"
    }
   },
   "outputs": [],
   "source": [
    "# Join the taxi zones with property sales\n",
    "zones_2019 = zones.merge(aggregated_sales_2019, left_on = ['Neighborhood'], right_on = ['NEIGHBORHOOD'], how = 'left')\n",
    "zones_2020 = zones.merge(aggregated_sales_2020, left_on = ['Neighborhood'], right_on = ['NEIGHBORHOOD'], how = 'left')\n",
    "\n",
    "# Fill na with Borough mean if the Neighborhood is not found\n",
    "for i in range(6, 11):\n",
    "    col = zones_2019.columns[i]\n",
    "    zones_2019[col] = zones_2019[col].fillna(zones_2019.groupby('Borough')[col].transform('mean'))\n",
    "    zones_2020[col] = zones_2020[col].fillna(zones_2020.groupby('Borough')[col].transform('mean'))\n",
    "    \n",
    "    \n",
    "selected_columns = ['LocationID',  \n",
    "                    'PRICE PER UNIT', \n",
    "                    'PRICE PER SQUARE FEET']\n",
    "\n",
    "renamed_columns = {'PRICE PER UNIT': 'Price_per_unit', \n",
    "                   'PRICE PER SQUARE FEET': 'Price_per_square_feet'}\n",
    "    \n",
    "zones_2019 = zones_2019[selected_columns].rename(columns = renamed_columns)\n",
    "zones_2020 = zones_2020[selected_columns].rename(columns = renamed_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:24.771379Z",
     "start_time": "2022-08-23T05:42:24.298704Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in population by neigborhood data and its shape file\n",
    "population = pd.read_csv(relative_directory + 'nyc_population_by_neighborhood.csv')\n",
    "population_sf = gpd.read_file(\"../data/raw/external_data_and_taxi_zones/nynta2010_22b/nynta2010.shp\")\n",
    "\n",
    "# read in taxi zones shape file\n",
    "zones_sf = gpd.read_file(\"../data/raw/external_data_and_taxi_zones/taxi_zones/taxi_zones.shp\")\n",
    "zones_sf['geometry'] = zones_sf['geometry'].to_crs(2830) # 2830 is the EPSG code for New York\n",
    "zones_gdf = gpd.GeoDataFrame(\n",
    "    pd.merge(zones, zones_sf, on='LocationID', how='inner')\n",
    ")\n",
    "zones_gdf = zones_gdf.drop_duplicates('LocationID') # Drop duplicated id\n",
    "\n",
    "# Convert the geometry shape to to latitude and longitude\n",
    "population_sf['geometry'] = population_sf['geometry'].to_crs(2830)\n",
    "\n",
    "# we will use only 2010 data\n",
    "population = population[population['Year'] == 2010]\n",
    "\n",
    "# Merge\n",
    "population_gdf = gpd.GeoDataFrame(\n",
    "    pd.merge(population, population_sf, left_on = 'NTA Code', right_on = 'NTACode', how='inner')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Since the metadata does not specify the unit of the areas, but we know that the area of New York is 783.8 km2. <br>\n",
    "By trying out a few units, we can deduce that the internal unit is square foot. The size of New York in square feet is about 8.43675e+9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:24.777507Z",
     "start_time": "2022-08-23T05:42:24.773315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York is 8.42299e+09 square feet large'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"New York is {population_gdf['Shape_Area'].sum():.6} square feet large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess population data (continue.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:24.786049Z",
     "start_time": "2022-08-23T05:42:24.779920Z"
    }
   },
   "outputs": [],
   "source": [
    "population_gdf['Shape_Area'] = population_gdf['Shape_Area'] / 27878400 # change square feet to square miles\n",
    "population_gdf = population_gdf[['NTA Code', 'Population', 'Shape_Area', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:25.743240Z",
     "start_time": "2022-08-23T05:42:24.788038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the interceptions of all area between neighborhood and service zones\n",
    "merged = gpd.overlay(zones_gdf, population_gdf, how = 'intersection', keep_geom_type = True)\n",
    "merged = merged[['LocationID', 'NTA Code', 'Shape_Area_2', \n",
    "                 'Population', 'geometry' \n",
    "                ]].rename(columns = {'Shape_Area_2': 'Area_in_square_miles', 'NTA Code': 'NTA_Code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:25.765011Z",
     "start_time": "2022-08-23T05:42:25.745396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assume that population in each NTA are evenly distributed\n",
    "\n",
    "# Merge again with the population_sf to calculate the proportion of intersection in NTA\n",
    "merged = pd.merge(merged, population_sf, \n",
    "                  left_on = 'NTA_Code', right_on = 'NTACode', \n",
    "                  how='inner', \n",
    "                  suffixes=('_merged', '_population'))\n",
    "\n",
    "merged['area_proportion'] = merged['geometry_merged'].area / merged['geometry_population'].area\n",
    "merged['Partial_Population'] = merged['Population'] * merged['area_proportion']\n",
    "merged['Population_By_LocationID'] = merged.groupby('LocationID')['Partial_Population'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:25.784962Z",
     "start_time": "2022-08-23T05:42:25.766839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finalise the preprocessing for population data\n",
    "zones_population = pd.merge(zones_gdf, merged, on = 'LocationID', how = 'left')\n",
    "\n",
    "zones_population['Density_per_hectare'] =  zones_population['Population_By_LocationID'] \\\n",
    "                                                    / zones_population['geometry'].area * 10000\n",
    "zones_population = zones_population[['LocationID', 'Population_By_LocationID', 'Density_per_hectare']]\n",
    "zones_population.drop_duplicates(inplace = True)\n",
    "zones_population.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:25.800997Z",
     "start_time": "2022-08-23T05:42:25.786918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine population infomation with taxi zones and property sales\n",
    "new_zones_2019 = pd.merge(zones_2019, zones_population, on = 'LocationID', how = 'inner')\n",
    "new_zones_2020 = pd.merge(zones_2020, zones_population, on = 'LocationID', how = 'inner')\n",
    "\n",
    "# Write out the files\n",
    "new_zones_2019.to_csv(\"../data/curated/new_zones_2019.csv\", index = False)\n",
    "new_zones_2020.to_csv(\"../data/curated/new_zones_2020.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:25.808905Z",
     "start_time": "2022-08-23T05:42:25.802422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Price_per_unit</th>\n",
       "      <th>Price_per_square_feet</th>\n",
       "      <th>Population_By_LocationID</th>\n",
       "      <th>Density_per_hectare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>658,500.00</td>\n",
       "      <td>599.66</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>303,035.71</td>\n",
       "      <td>294.48</td>\n",
       "      <td>176.83</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>466,184.68</td>\n",
       "      <td>307.00</td>\n",
       "      <td>28,902.34</td>\n",
       "      <td>97.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>525,469.22</td>\n",
       "      <td>459.57</td>\n",
       "      <td>25,123.60</td>\n",
       "      <td>335.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>435,373.33</td>\n",
       "      <td>310.58</td>\n",
       "      <td>25,233.23</td>\n",
       "      <td>53.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID  Price_per_unit  Price_per_square_feet  \\\n",
       "0           1      658,500.00                 599.66   \n",
       "1           2      303,035.71                 294.48   \n",
       "2           3      466,184.68                 307.00   \n",
       "3           4      525,469.22                 459.57   \n",
       "4           5      435,373.33                 310.58   \n",
       "\n",
       "   Population_By_LocationID  Density_per_hectare  \n",
       "0                       nan                  nan  \n",
       "1                    176.83                 0.13  \n",
       "2                 28,902.34                97.81  \n",
       "3                 25,123.60               335.82  \n",
       "4                 25,233.23                53.70  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the processed data\n",
    "new_zones_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About how to get this data\n",
    "1. Open https://www.visualcrossing.com/weather/weather-data-services\n",
    "2. Create a free acount with 1,000 rows queries available\n",
    "3. Summit the query for New York City weather from January 1st to December 31st of 2019\n",
    "4. Summit the query for New York City weather from January 1st to February 29th of 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:25.836891Z",
     "start_time": "2022-08-23T05:42:25.810405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the weather data\n",
    "weather_2019 = pd.read_csv(\"../data/raw/external_data_and_taxi_zones/nyc_weather_2019_Jan_to_Dec.csv\",\n",
    "                           parse_dates = ['datetime', ])\n",
    "weather_2020 = pd.read_csv(\"../data/raw/external_data_and_taxi_zones/nyc_weather_2020_Jan_to_Feb.csv\",\n",
    "                           parse_dates = ['datetime', ])\n",
    "\n",
    "# Using feelslike temperature is more suitable in the case of tip amount analysis\n",
    "selected_columns = ['month', 'feelslike', \n",
    "                    'feelslikemax', 'feelslikemin',\n",
    "                    'precip', 'precipcover', 'snow', \n",
    "                    'snowdepth', 'windspeed', \n",
    "                    'cloudcover', 'visibility'\n",
    "                   ]\n",
    "\n",
    "def transform_weather_data(weather):\n",
    "    weather['month'] = weather['datetime'].dt.month\n",
    "    weather = weather[selected_columns]\n",
    "    weather = weather.groupby('month', as_index = False).mean()\n",
    "    return weather\n",
    "\n",
    "weather_2019 = transform_weather_data(weather_2019)\n",
    "weather_2020 = transform_weather_data(weather_2020)\n",
    "\n",
    "\n",
    "# Write out the files\n",
    "weather_2019.to_csv(\"../data/curated/weather_2019.csv\", index = False)\n",
    "weather_2020.to_csv(\"../data/curated/weather_2020.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:25.847819Z",
     "start_time": "2022-08-23T05:42:25.838816Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>feelslike</th>\n",
       "      <th>feelslikemax</th>\n",
       "      <th>feelslikemin</th>\n",
       "      <th>precip</th>\n",
       "      <th>precipcover</th>\n",
       "      <th>snow</th>\n",
       "      <th>snowdepth</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>visibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-4.05</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-9.15</td>\n",
       "      <td>3.11</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>30.88</td>\n",
       "      <td>50.83</td>\n",
       "      <td>14.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>2.71</td>\n",
       "      <td>12.20</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>29.36</td>\n",
       "      <td>47.98</td>\n",
       "      <td>14.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.45</td>\n",
       "      <td>7.18</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>3.03</td>\n",
       "      <td>9.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.49</td>\n",
       "      <td>24.55</td>\n",
       "      <td>47.07</td>\n",
       "      <td>14.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11.83</td>\n",
       "      <td>16.54</td>\n",
       "      <td>7.49</td>\n",
       "      <td>3.46</td>\n",
       "      <td>12.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.15</td>\n",
       "      <td>53.10</td>\n",
       "      <td>14.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>16.29</td>\n",
       "      <td>20.67</td>\n",
       "      <td>12.36</td>\n",
       "      <td>5.06</td>\n",
       "      <td>17.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.59</td>\n",
       "      <td>58.09</td>\n",
       "      <td>13.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  feelslike  feelslikemax  feelslikemin  precip  precipcover  snow  \\\n",
       "0      1      -4.05          0.73         -9.15    3.11        10.62  0.10   \n",
       "1      2      -1.81          2.82         -6.40    2.71        12.20  0.48   \n",
       "2      3       2.45          7.18         -2.00    3.03         9.95  0.55   \n",
       "3      4      11.83         16.54          7.49    3.46        12.22  0.00   \n",
       "4      5      16.29         20.67         12.36    5.06        17.61  0.00   \n",
       "\n",
       "   snowdepth  windspeed  cloudcover  visibility  \n",
       "0       0.10      30.88       50.83       14.80  \n",
       "1       0.36      29.36       47.98       14.30  \n",
       "2       1.49      24.55       47.07       14.75  \n",
       "3       0.00      24.15       53.10       14.61  \n",
       "4       0.00      21.59       58.09       13.68  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the processed data\n",
    "weather_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:26.008560Z",
     "start_time": "2022-08-23T05:42:25.849599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data for training\n",
    "pu_aggregated_result_2019 = pd.read_parquet('../data/curated/pu_aggregated_result_2019')\n",
    "do_aggregated_result_2019 = pd.read_parquet('../data/curated/do_aggregated_result_2019')\n",
    "\n",
    "# Data for testing\n",
    "pu_aggregated_result_2020 = pd.read_parquet('../data/curated/pu_aggregated_result_2020')\n",
    "do_aggregated_result_2020 = pd.read_parquet('../data/curated/do_aggregated_result_2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T05:42:26.219980Z",
     "start_time": "2022-08-23T05:42:26.010274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "def merge_and_save_df(aggdata, zone, weather, filename):\n",
    "    if \"PULocationID\" in aggdata:\n",
    "        key1 = \"PULocationID\"\n",
    "        key2 = \"PUMonth\"\n",
    "    else:\n",
    "        key1 = \"DOLocationID\"\n",
    "        key2 = \"DOMonth\"\n",
    "    temp = pd.merge(aggdata, zone, \n",
    "                    left_on = key1, right_on = \"LocationID\",\n",
    "                    how = 'inner')\n",
    "    final = pd.merge(temp, weather, \n",
    "                     left_on = key2, right_on = \"month\",\n",
    "                     how = 'inner')\n",
    "    final = final.drop(columns = [key1, key2])\n",
    "    # Add a column indicates total trip in a year for a location\n",
    "    final = final.join(final.groupby('LocationID')['trip_count'].sum(), \n",
    "                       rsuffix='_total', \n",
    "                       on = \"LocationID\")\n",
    "    # Add a column indicates total trip in a month\n",
    "    final = final.join(final.groupby(\"month\")['trip_count'].sum(), \n",
    "                       rsuffix='_in_month', \n",
    "                       on = \"month\")\n",
    "    final.to_csv(f\"../data/curated/{filename}.csv\", index = False)\n",
    "\n",
    "merge_and_save_df(pu_aggregated_result_2019, new_zones_2019, weather_2019, 'pu_2019')\n",
    "merge_and_save_df(pu_aggregated_result_2020, new_zones_2020, weather_2020, 'pu_2020')\n",
    "merge_and_save_df(do_aggregated_result_2019, new_zones_2019, weather_2019, 'do_2019')\n",
    "merge_and_save_df(do_aggregated_result_2020, new_zones_2020, weather_2020, 'do_2020')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
