{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T16:03:44.859423Z",
     "start_time": "2022-08-13T16:03:44.854860Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import geopandas as gpd\n",
    "import openpyxl\n",
    "import os\n",
    "random.seed(10)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T16:03:45.278107Z",
     "start_time": "2022-08-13T16:03:45.270149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in taxi_zones lookup table\n",
    "zones = pd.read_csv(\"../data/raw/external_data_and_taxi_zones/taxi_zone_lookup.csv\")\n",
    "zones = zones.drop([263, 264]) # drop the unknown zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess property sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T16:04:33.326865Z",
     "start_time": "2022-08-13T16:03:46.250559Z"
    }
   },
   "outputs": [],
   "source": [
    "relative_directory = '../data/raw/external_data_and_taxi_zones/'\n",
    "\n",
    "all_files = os.listdir(\"../data/raw/external_data_and_taxi_zones/\")    \n",
    "property_sale_files = list(filter(lambda x: x.endswith('.xlsx'), all_files))\n",
    "\n",
    "header = ['BOROUGH', 'NEIGHBORHOOD','BUILDING CLASS CATEGORY', \n",
    "          'TAX CLASS AT PRESENT', 'BLOCK', 'LOT', \n",
    "          'EASE-MENT', 'BUILDING CLASS AT PRESENT', 'ADDRESS',\n",
    "          'APARTMENT NUMBER', 'ZIP CODE', 'RESIDENTIAL UNITS',\n",
    "          'COMMERCIAL UNITS', 'TOTAL UNITS', 'LAND SQUARE FEET',\n",
    "          'GROSS SQUARE FEET', 'YEAR BUILT', 'TAX CLASS AT TIME OF SALE',\n",
    "          'BUILDING CLASS AT TIME OF SALE', 'SALE PRICE', 'SALE DATE']\n",
    "\n",
    "\n",
    "file_names = [relative_directory + file for file in property_sale_files]\n",
    "\n",
    "\n",
    "def read_xlxs_for_mapping(data):\n",
    "    return pd.read_excel(data, \n",
    "                        names = header, \n",
    "                        parse_dates = ['SALE DATE', ],\n",
    "                        engine = 'openpyxl')\n",
    "\n",
    "property_sales = pd.concat(map(read_xlxs_for_mapping, file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:39:32.319081Z",
     "start_time": "2022-08-13T15:39:32.168250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 206017 entries, 0 to 21716\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count   Dtype \n",
      "---  ------                          --------------   ----- \n",
      " 0   BOROUGH                         152654 non-null  object\n",
      " 1   NEIGHBORHOOD                    152614 non-null  object\n",
      " 2   BUILDING CLASS CATEGORY         152614 non-null  object\n",
      " 3   TAX CLASS AT PRESENT            152383 non-null  object\n",
      " 4   BLOCK                           152614 non-null  object\n",
      " 5   LOT                             152614 non-null  object\n",
      " 6   EASE-MENT                       10 non-null      object\n",
      " 7   BUILDING CLASS AT PRESENT       152383 non-null  object\n",
      " 8   ADDRESS                         152614 non-null  object\n",
      " 9   APARTMENT NUMBER                33410 non-null   object\n",
      " 10  ZIP CODE                        152589 non-null  object\n",
      " 11  RESIDENTIAL UNITS               122566 non-null  object\n",
      " 12  COMMERCIAL UNITS                122566 non-null  object\n",
      " 13  TOTAL UNITS                     122566 non-null  object\n",
      " 14  LAND SQUARE FEET                122566 non-null  object\n",
      " 15  GROSS SQUARE FEET               122566 non-null  object\n",
      " 16  YEAR BUILT                      141743 non-null  object\n",
      " 17  TAX CLASS AT TIME OF SALE       152614 non-null  object\n",
      " 18  BUILDING CLASS AT TIME OF SALE  152614 non-null  object\n",
      " 19  SALE PRICE                      152614 non-null  object\n",
      " 20  SALE DATE                       206012 non-null  object\n",
      "dtypes: object(21)\n",
      "memory usage: 34.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "property_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:53:39.492608Z",
     "start_time": "2022-08-13T15:53:39.485481Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data, columns):\n",
    "    '''\n",
    "    remove outliers from data that is 1.5 iqr away from q1 or q3\n",
    "    '''\n",
    "    new_data = data.copy()\n",
    "    q1 = np.array([np.quantile(data[column], 0.25) for column in columns])\n",
    "    q3 = np.array([np.quantile(data[column], 0.75) for column in columns])\n",
    "    iqr = q3 - q1\n",
    "    for i in range(len(columns)):\n",
    "        column = columns[i]\n",
    "        new_data = new_data[(new_data[column] > q1[i] - 1.5 * iqr[i]) & (new_data[column] < q3[i] + 1.5 * iqr[i])]\n",
    "    return new_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T16:02:06.957905Z",
     "start_time": "2022-08-13T16:02:06.484499Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-20e9759e91ba>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  property_sales.dropna(how='any', subset=['BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS AT PRESENT',\n",
      "<ipython-input-65-20e9759e91ba>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  property_sales['BOROUGH'] = property_sales['BOROUGH'].astype(int)\n",
      "<ipython-input-65-20e9759e91ba>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  property_sales['SALE DATE'] = pd.to_datetime(property_sales['SALE DATE'])\n",
      "<ipython-input-65-20e9759e91ba>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  property_sales['SALE PRICE'] = pd.to_numeric(property_sales['SALE PRICE'])\n",
      "<ipython-input-65-20e9759e91ba>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  property_sales['GROSS SQUARE FEET'] = pd.to_numeric(property_sales['GROSS SQUARE FEET'])\n",
      "<ipython-input-65-20e9759e91ba>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  property_sales['TOTAL UNITS'] = pd.to_numeric(property_sales['TOTAL UNITS'])\n",
      "<ipython-input-65-20e9759e91ba>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  property_sales['BOROUGH'] = property_sales['BOROUGH'].apply(lambda x: borough_dict[x])\n"
     ]
    }
   ],
   "source": [
    "# Remove irrelevant rows\n",
    "property_sales = property_sales[property_sales['BOROUGH'].isin(['1', '2', '3', '4', '5'])]\n",
    "\n",
    "# Drop rows with NaN valus in required fields\n",
    "property_sales.dropna(how='any', subset=['BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS AT PRESENT',\n",
    "                                         'TOTAL UNITS', 'GROSS SQUARE FEET', 'BUILDING CLASS AT TIME OF SALE', \n",
    "                                         'SALE PRICE', 'SALE DATE'], inplace=True)\n",
    "\n",
    "# Change data type\n",
    "property_sales['BOROUGH'] = property_sales['BOROUGH'].astype(int)\n",
    "property_sales['SALE DATE'] = pd.to_datetime(property_sales['SALE DATE'])\n",
    "property_sales['SALE PRICE'] = pd.to_numeric(property_sales['SALE PRICE'])\n",
    "property_sales['GROSS SQUARE FEET'] = pd.to_numeric(property_sales['GROSS SQUARE FEET'])\n",
    "property_sales['TOTAL UNITS'] = pd.to_numeric(property_sales['TOTAL UNITS'])\n",
    "\n",
    "\n",
    "# According to property sales data, 1 for man, 2 for bronx, 3 for brooklyn, 4 for queens, 5 for staten island\n",
    "borough_dict = {4: 'Queens', 2: 'Bronx', 1: 'Manhattan', 5: 'Staten Island', 3: 'Brooklyn'}\n",
    "property_sales['BOROUGH'] = property_sales['BOROUGH'].apply(lambda x: borough_dict[x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter data that fits our analysis\n",
    "# Type ABCD are family dwellings and apartments, H is hotel\n",
    "condition = (property_sales['SALE DATE'] > '01-01-2019') & (property_sales['SALE DATE'] <= '29-02-2020') &\\\n",
    "            (property_sales['SALE PRICE'] > 0) &\\\n",
    "            (property_sales['GROSS SQUARE FEET'] > 0) &\\\n",
    "            (property_sales['TOTAL UNITS'] > 0) &\\\n",
    "            (property_sales['BUILDING CLASS AT PRESENT'] == property_sales['BUILDING CLASS AT TIME OF SALE']) &\\\n",
    "            (property_sales['BUILDING CLASS AT TIME OF SALE'].str.contains('^[ABCDH]', regex=True))\n",
    "\n",
    "\n",
    "property_sales = property_sales.loc[condition]\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "property_sales.drop(columns = ['BUILDING CLASS CATEGORY', 'TAX CLASS AT PRESENT', 'BLOCK', \n",
    "                               'LOT', 'EASE-MENT', 'BUILDING CLASS AT PRESENT', \n",
    "                               'ADDRESS', 'APARTMENT NUMBER', 'ZIP CODE', \n",
    "                               'RESIDENTIAL UNITS', 'COMMERCIAL UNITS', \n",
    "                               'LAND SQUARE FEET', 'YEAR BUILT', 'TAX CLASS AT TIME OF SALE'], inplace=True)\n",
    "\n",
    "\n",
    "# Add 'PRICE PER UNIT' and 'PRICE PER SQUARE FEET' as features\n",
    "\n",
    "property_sales['PRICE PER UNIT'] = property_sales['SALE PRICE'] / property_sales['TOTAL UNITS']\n",
    "\n",
    "property_sales['PRICE PER SQUARE FEET'] = property_sales['SALE PRICE'] / property_sales['GROSS SQUARE FEET']\n",
    "\n",
    "property_sales = remove_outliers(property_sales, ['TOTAL UNITS',\n",
    "                                                  'GROSS SQUARE FEET',\n",
    "                                                  'SALE PRICE',\n",
    "                                                  'PRICE PER UNIT',\n",
    "                                                  'PRICE PER SQUARE FEET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T16:02:24.297485Z",
     "start_time": "2022-08-13T16:02:24.273601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "\n",
    "aggregated_sales_2019 = property_sales[(property_sales['SALE DATE'] > '01-01-2019') & \\\n",
    "                                       (property_sales['SALE DATE'] <= '31-12-2019')] \\\n",
    "                        .groupby(['NEIGHBORHOOD'], as_index = False).mean()\n",
    "\n",
    "aggregated_sales_2020 = property_sales[(property_sales['SALE DATE'] > '01-01-2020') & \\\n",
    "                                       (property_sales['SALE DATE'] <= '29-02-2020')] \\\n",
    "                        .groupby(['NEIGHBORHOOD'], as_index = False).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:43:39.880538Z",
     "start_time": "2022-08-13T15:43:39.280241Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data linkage has 234 matches'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Link taxi zones data to property sales data based on their name of locations\n",
    "# for taxi zones data is the 'Zone', for property sales data is the 'Neighborhood'\n",
    "\n",
    "def t(name):\n",
    "    \"\"\"\n",
    "    return a transformed zone name that is splitted into several parts\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    name = re.sub('[\\/\\-0-9()]+', ' ', name)\n",
    "    names = name.split()\n",
    "    return names\n",
    "\n",
    "\n",
    "\n",
    "def link(name1, name_list_2, threshold_confidence):\n",
    "    \"\"\"\n",
    "    return a possible match of name1 from name_list_2\n",
    "    \"\"\"\n",
    "    confident_pairs = [[(name1, name2),\n",
    "                        len(set(t(name1)).intersection(t(name2))) \n",
    "                        / len(t(name1))\n",
    "                        ] \\\n",
    "                        for name2 in name_list_2]\n",
    "    max_confidence = max([confidence for pair, confidence in confident_pairs])\n",
    "    most_confident_pairs = [pair for pair, confidence in confident_pairs if confidence == max_confidence]\n",
    "    number_of_most_confident_pairs = len(most_confident_pairs)\n",
    "    if max_confidence >= threshold_confidence:\n",
    "        # we randomly choose one pair, because if two zones have similar names they are likely \n",
    "        # to have similar locations\n",
    "        return most_confident_pairs[random.randint(0, len(most_confident_pairs) - 1)][1]\n",
    "\n",
    "\n",
    "\n",
    "# Find the corresponding neighborhood to each taxi zone\n",
    "zones_dict = {}\n",
    "for zone in zones['Zone'].unique():\n",
    "    zones_dict[zone] = link(zone, \n",
    "                            property_sales['NEIGHBORHOOD'].unique(), \n",
    "                            threshold_confidence = 0.25) # set a low threshold confidence to get high recall\n",
    "\n",
    "\n",
    "\n",
    "zones['Neighborhood'] = zones['Zone'].apply(lambda x: zones_dict[x])\n",
    "f'Data linkage has {len([i for i in zones_dict.values() if i != None])} matches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:43:41.176353Z",
     "start_time": "2022-08-13T15:43:41.164226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "      <th>Neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "      <td>AIRPORT LA GUARDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>JAMAICA BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>PELHAM GARDENS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>CITY ISLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>ARDEN HEIGHTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone  \\\n",
       "0           1            EWR           Newark Airport          EWR   \n",
       "1           2         Queens              Jamaica Bay    Boro Zone   \n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone   \n",
       "3           4      Manhattan            Alphabet City  Yellow Zone   \n",
       "4           5  Staten Island            Arden Heights    Boro Zone   \n",
       "\n",
       "         Neighborhood  \n",
       "0  AIRPORT LA GUARDIA  \n",
       "1         JAMAICA BAY  \n",
       "2      PELHAM GARDENS  \n",
       "3         CITY ISLAND  \n",
       "4       ARDEN HEIGHTS  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some errors still exist but not a big problem\n",
    "zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:42:21.638087Z",
     "start_time": "2022-08-13T15:42:21.601177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Join the taxi zones with property sales\n",
    "zones_2019 = zones.merge(aggregated_sales_2019, left_on = ['Neighborhood'], right_on = ['NEIGHBORHOOD'], how = 'left')\n",
    "zones_2020 = zones.merge(aggregated_sales_2020, left_on = ['Neighborhood'], right_on = ['NEIGHBORHOOD'], how = 'left')\n",
    "\n",
    "# Fill na with Borough mean if the Neighborhood is not found\n",
    "for i in range(6, 11):\n",
    "    col = zones_2019.columns[i]\n",
    "    zones_2019[col] = zones_2019[col].fillna(zones_2019.groupby('Borough')[col].transform('mean'))\n",
    "    zones_2020[col] = zones_2020[col].fillna(zones_2020.groupby('Borough')[col].transform('mean'))\n",
    "    \n",
    "    \n",
    "selected_columns = ['LocationID', 'TOTAL UNITS', \n",
    "                    'GROSS SQUARE FEET', 'SALE PRICE', \n",
    "                    'PRICE PER UNIT', 'PRICE PER SQUARE FEET']\n",
    "\n",
    "renamed_columns = {'TOTAL UNITS': 'Total_units', \n",
    "                   'GROSS SQUARE FEET': 'Gross_square_feet', \n",
    "                   'SALE PRICE': 'Sale_price',\n",
    "                   'PRICE PER UNIT': 'Price_per_unit', \n",
    "                   'PRICE PER SQUARE FEET': 'Price_per_square_feet'}\n",
    "    \n",
    "zones_2019 = zones_2019[selected_columns].rename(columns = renamed_columns)\n",
    "zones_2020 = zones_2020[selected_columns].rename(columns = renamed_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.502558Z",
     "start_time": "2022-08-13T15:31:30.416Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in population by neigborhood data and its shape file\n",
    "population = pd.read_csv(relative_directory + 'nyc_population_by_neighborhood.csv')\n",
    "population_sf = gpd.read_file(\"../data/raw/external_data_and_taxi_zones/nynta2010_22b/nynta2010.shp\")\n",
    "\n",
    "# read in taxi zones shape file\n",
    "zones_sf = gpd.read_file(\"../data/raw/external_data_and_taxi_zones/taxi_zones/taxi_zones.shp\")\n",
    "zones_sf['geometry'] = zones_sf['geometry'].to_crs(2830) # 2830 is the EPSG code for New York\n",
    "zones_gdf = gpd.GeoDataFrame(\n",
    "    pd.merge(zones, zones_sf, on='LocationID', how='inner')\n",
    ")\n",
    "zones_gdf = zones_gdf.drop_duplicates('LocationID') # Drop duplicated id\n",
    "\n",
    "# Convert the geometry shape to to latitude and longitude\n",
    "population_sf['geometry'] = population_sf['geometry'].to_crs(2830)\n",
    "\n",
    "# we will use only 2010 data\n",
    "population = population[population['Year'] == 2010]\n",
    "\n",
    "# Merge\n",
    "population_gdf = gpd.GeoDataFrame(\n",
    "    pd.merge(population, population_sf, left_on = 'NTA Code', right_on = 'NTACode', how='inner')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Since the metadata does not specify the unit of the areas, but we know that the area of New York is 783.8 km2. <br>\n",
    "By trying out a few units, we can deduce that the internal unit is square foot. The size of New York in square feet is about 8.43675e+9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.503959Z",
     "start_time": "2022-08-13T15:31:30.417Z"
    }
   },
   "outputs": [],
   "source": [
    "f\"New York is {population_gdf['Shape_Area'].sum():.6} square feet large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess population data (continue.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.505321Z",
     "start_time": "2022-08-13T15:31:30.418Z"
    }
   },
   "outputs": [],
   "source": [
    "population_gdf['Shape_Area'] = population_gdf['Shape_Area'] / 27878400 # change square feet to square miles\n",
    "population_gdf = population_gdf[['NTA Code', 'Population', 'Shape_Area', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.506521Z",
     "start_time": "2022-08-13T15:31:30.419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the interceptions of all area between neighborhood and service zones\n",
    "merged = gpd.overlay(zones_gdf, population_gdf, how = 'intersection', keep_geom_type = True)\n",
    "merged = merged[['LocationID', 'NTA Code', 'Shape_Area_2', \n",
    "                 'Population', 'geometry' \n",
    "                ]].rename(columns = {'Shape_Area_2': 'Area_in_square_miles', 'NTA Code': 'NTA_Code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.507813Z",
     "start_time": "2022-08-13T15:31:30.420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assume that population in each NTA are evenly distributed\n",
    "\n",
    "# Merge again with the population_sf to calculate the proportion of intersection in NTA\n",
    "merged = pd.merge(merged, population_sf, \n",
    "                  left_on = 'NTA_Code', right_on = 'NTACode', \n",
    "                  how='inner', \n",
    "                  suffixes=('_merged', '_population'))\n",
    "\n",
    "merged['area_proportion'] = merged['geometry_merged'].area / merged['geometry_population'].area\n",
    "merged['Partial_Population'] = merged['Population'] * merged['area_proportion']\n",
    "merged['Population_By_LocationID'] = merged.groupby('LocationID')['Partial_Population'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.508951Z",
     "start_time": "2022-08-13T15:31:30.421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finalise the preprocessing for population data\n",
    "zones_population = pd.merge(zones_gdf, merged, on = 'LocationID', how = 'left')\n",
    "\n",
    "zones_population['Density_per_square_metre'] =  zones_population['Population_By_LocationID'] \\\n",
    "                                                / zones_population['geometry'].area\n",
    "zones_population = zones_population[['LocationID', 'Population_By_LocationID', 'Density_per_square_metre']]\n",
    "zones_population.drop_duplicates(inplace = True)\n",
    "zones_population.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.509964Z",
     "start_time": "2022-08-13T15:31:30.422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine population infomation with taxi zones and property sales\n",
    "new_zones_2019 = pd.merge(zones_2019, zones_population, on = 'LocationID', how = 'inner')\n",
    "new_zones_2020 = pd.merge(zones_2020, zones_population, on = 'LocationID', how = 'inner')\n",
    "\n",
    "# Write out the files\n",
    "new_zones_2019.to_csv(\"../data/curated/new_zones_2019.csv\", index = False)\n",
    "new_zones_2020.to_csv(\"../data/curated/new_zones_2020.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.510959Z",
     "start_time": "2022-08-13T15:31:30.423Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the processed data\n",
    "# new_zones_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## about how to get this data\n",
    "1. Open https://www.visualcrossing.com/weather/weather-data-services\n",
    "2. Create a free acount with 1,000 rows queries available\n",
    "3. Summit the query for New York City weather from January 1st to December 31st of 2019\n",
    "4. Summit the query for New York City weather from January 1st to February 29th of 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.512256Z",
     "start_time": "2022-08-13T15:31:30.425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the weather data\n",
    "weather_2019 = pd.read_csv(\"../data/raw/external_data_and_taxi_zones/nyc_weather_2019_Jan_to_Dec.csv\",\n",
    "                           parse_dates = ['datetime', ])\n",
    "weather_2020 = pd.read_csv(\"../data/raw/external_data_and_taxi_zones/nyc_weather_2020_Jan_to_Feb.csv\",\n",
    "                           parse_dates = ['datetime', ])\n",
    "\n",
    "# Using feelslike temperature is more suitable in the case of tip amount analysis\n",
    "selected_columns = ['month', 'day_of_month', 'feelslike', \n",
    "                    'feelslikemax', 'feelslikemin', 'feelslike_temp_diff',\n",
    "                    'precip', 'precipcover', 'snow', \n",
    "                    'snowdepth', 'windspeed', 'cloudcover', 'visibility'\n",
    "                   ]\n",
    "\n",
    "def transform_weather_data(weather):\n",
    "    weather['day_of_month'] = weather['datetime'].dt.day\n",
    "    weather['month'] = weather['datetime'].dt.month\n",
    "    weather['feelslike_temp_diff'] = weather['feelslikemax'] - weather['feelslikemin']\n",
    "    weather = weather[selected_columns]\n",
    "    return weather\n",
    "\n",
    "weather_2019 = transform_weather_data(weather_2019)\n",
    "weather_2020 = transform_weather_data(weather_2020)\n",
    "\n",
    "\n",
    "# Write out the files\n",
    "weather_2019.to_csv(\"../data/curated/weather_2019.csv\", index = False)\n",
    "weather_2020.to_csv(\"../data/curated/weather_2020.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-13T15:32:17.513296Z",
     "start_time": "2022-08-13T15:31:30.426Z"
    }
   },
   "outputs": [],
   "source": [
    "# View the processed data\n",
    "# weather_2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
